{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense,Input,GRU,Dropout,concatenate,Permute,Reshape,Lambda,RepeatVector,merge,MaxPooling1D,Embedding,Activation,Conv1D,Flatten\n",
    "import pickle\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import *\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error,roc_curve\n",
    "from keras.backend import eval\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_hidden = 200\n",
    "drop_out = 0.2\n",
    "time_length = 365\n",
    "n_emb = 100\n",
    "n_feature = n_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_model():\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "    \n",
    "    gru_h = GRU(n_hidden,activation='tanh', recurrent_activation='sigmoid', \n",
    "                use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                recurrent_initializer='orthogonal', bias_initializer='zeros')(X_input)\n",
    "    gru_h = Dropout(drop_out)(gru_h)\n",
    "    comb = concatenate([gru_h,x_dmgs])\n",
    "    h_t = Dense(1,activation='sigmoid')(comb)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs = h_t)\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pooling_model(ptype):\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "    \n",
    "    gru_h = GRU(n_hidden,activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                recurrent_initializer='orthogonal', bias_initializer='zeros',return_sequences=True)(X_input)\n",
    "    \n",
    "    gru_out = Lambda(lambda x: x[:,-1,:])(gru_h)\n",
    "    gru_out = Reshape((n_hidden,))(gru_out)\n",
    "    \n",
    "    if ptype == 'max':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru])\n",
    "    \n",
    "    elif ptype == 'avg':\n",
    "        avg_gru = Lambda(lambda x: K.mean(x,axis=-2))(gru_h)\n",
    "        avg_gru = Reshape((n_hidden,))(avg_gru)\n",
    "        gru_h = concatenate([gru_out,avg_gru])\n",
    "    \n",
    "    elif ptype == 'min':\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,min_gru])\n",
    "        \n",
    "    elif ptype == 'bpv':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        avg_gru = Lambda(lambda x: K.mean(x,axis=-2))(gru_h)\n",
    "        avg_gru = Reshape((n_hidden,))(avg_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru,avg_gru,min_gru])\n",
    "        \n",
    "    elif ptype == 'maxmin':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru,min_gru])\n",
    "        \n",
    "    #gru_h = concatenate([gru_out,max_gru,avg_gru])\n",
    "    gru_h = Dropout(drop_out)(gru_h)\n",
    "    \n",
    "    comb = concatenate([gru_h,x_dmgs])\n",
    "    h_t = Dense(1,activation='sigmoid')(comb)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs = h_t)\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘rnn_emb_553_one_year/’: File exists\n",
      "mkdir: cannot create directory ‘rnn_emb_pooling_553_one_year/’: File exists\n",
      "[22 23 24 25 26 27 28 29 30 31 32]\n",
      "0 2018-08-07 22:52:26.737751\n",
      "1 2018-08-07 22:53:09.068393\n",
      "2 2018-08-07 22:53:52.913804\n",
      "3 2018-08-07 22:54:21.438501\n",
      "4 2018-08-07 22:55:05.335680\n",
      "5 2018-08-07 22:55:43.478158\n",
      "6 2018-08-07 22:56:11.643667\n",
      "7 2018-08-07 22:56:59.484769\n",
      "8 2018-08-07 22:57:35.619467\n",
      "9 2018-08-07 22:58:06.060391\n",
      "10 2018-08-07 22:58:56.635421\n",
      "11 2018-08-07 22:59:25.806267\n",
      "12 2018-08-07 23:00:12.220971\n",
      "13 2018-08-07 23:01:03.962377\n",
      "14 2018-08-07 23:01:32.275092\n",
      "15 2018-08-07 23:02:15.514839\n",
      "16 2018-08-07 23:03:05.206424\n",
      "17 2018-08-07 23:03:33.698401\n",
      "18 2018-08-07 23:04:08.206155\n",
      "19 2018-08-07 23:04:54.827373\n"
     ]
    }
   ],
   "source": [
    "bsize = 500\n",
    "max_epoch = 8\n",
    "model_folder0 = 'rnn_emb_553_one_year/'\n",
    "model_folder1 = 'rnn_emb_pooling_553_one_year/'\n",
    "\n",
    "!mkdir $model_folder0\n",
    "!mkdir $model_folder1\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_counter = 0\n",
    "\n",
    "for train_index, test_index in kf.split(np.arange(54)):\n",
    "    cv_counter += 1\n",
    "    if cv_counter > 2:\n",
    "        print(test_index)\n",
    "        with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "            with open(model_folder1+'cv_results.txt','a') as out1:\n",
    "                out0.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "                out1.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "        model0 = get_rnn_model()\n",
    "        model1 = get_pooling_model('bpv')\n",
    "        val_loss_list0,val_loss_list1,val_loss_list2,val_loss_list3 = [],[],[],[]\n",
    "\n",
    "        for k in range(max_epoch):\n",
    "            train_loss0,train_loss1,train_loss2,train_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "            val_loss0,val_loss1,val_loss2,val_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "            y_test_all0,y_test_all1,y_test_all2,y_test_all3 = None,None,None,None\n",
    "            y_pred_all0,y_pred_all1,y_pred_all2,y_pred_all3 = None,None,None,None\n",
    "\n",
    "            for i in train_index:\n",
    "                loadata = np.load('embedded_x/emb'+str(i)+'.npz')\n",
    "                dmg_data = np.load('Data/additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                x_dmg=dmg_data['values']\n",
    "                if i == 53:\n",
    "                    dmg_53 = np.zeros((9000,4))\n",
    "                    for mm in range(9000):\n",
    "                        dmg_53[mm,:] = x_dmg[mm]\n",
    "                    x_dmg = dmg_53\n",
    "                x_dmg[:,2] = abs(x_dmg[:,3]-2)\n",
    "                x_dmg = x_dmg[:,:3]\n",
    "                x = loadata['x']\n",
    "                y = loadata['y']\n",
    "                x = x[:,time_length:,:]\n",
    "                #x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                for j in range(0,x_dmg.shape[0],bsize):\n",
    "                    batch_loss0 = model0.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                    train_loss0 = np.add(train_loss0,batch_loss0)\n",
    "                    batch_loss1 = model1.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                    train_loss1 = np.add(train_loss1,batch_loss1)\n",
    "                print(i, datetime.now())\n",
    "\n",
    "            model0.save(model_folder0+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "            model1.save(model_folder1+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "\n",
    "            train_loss0 = np.divide(train_loss0,(len(train_index)*10000/bsize))\n",
    "            train_loss1 = np.divide(train_loss1,(len(train_index)*10000/bsize))\n",
    "\n",
    "            print('model0:','epoch',k,'train_loss:',train_loss0)\n",
    "            print('model1:','epoch',k,'train_loss:',train_loss1)\n",
    "            with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "                with open(model_folder1+'cv_results.txt','a') as out1:\n",
    "                    out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss0)])+'\\n')\n",
    "                    out1.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss1)])+'\\n')\n",
    "\n",
    "\n",
    "            if k > max_epoch-3:\n",
    "                for i in test_index:\n",
    "                    loadata = np.load('embedded_x/emb'+str(i)+'.npz')\n",
    "                    x=loadata['x']\n",
    "                    y=loadata['y']\n",
    "                    x = x[:,time_length:,:]\n",
    "                    #x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                    dmg_data = np.load('Data/additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                    x_val_dmg=dmg_data['values']\n",
    "                    if i == 53:\n",
    "                        dmg_53 = np.zeros((9000,4))\n",
    "                        for mm in range(9000):\n",
    "                            dmg_53[mm,:] = x_val_dmg[mm]\n",
    "                        x_val_dmg = dmg_53\n",
    "                        x = x[:9000]\n",
    "                        y = y[:9000]\n",
    "                    x_val_dmg[:,2] = abs(x_val_dmg[:,3]-2)\n",
    "                    x_val_dmg = x_val_dmg[:,:3]\n",
    "                    y_test_all = (y[:,0] if i==test_index[0] else np.append(y_test_all, y[:,0]))\n",
    "                    batch_loss0 = model0.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                    batch_loss1 = model1.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "\n",
    "                    val_loss0 = np.add(val_loss0,batch_loss0)\n",
    "                    val_loss1 = np.add(val_loss1,batch_loss1)\n",
    "\n",
    "                    print(i, datetime.now())\n",
    "                    y_pred0 = model0.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                    y_pred1 = model1.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                    np.savez_compressed(model_folder0+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred0)\n",
    "                    np.savez_compressed(model_folder1+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred1)\n",
    "                    y_pred_all0 = (y_pred0 if i==test_index[0] else np.append(y_pred_all0, y_pred0))\n",
    "                    y_pred_all1 = (y_pred1 if i==test_index[0] else np.append(y_pred_all1, y_pred1))\n",
    "\n",
    "                val_loss0 = np.divide(val_loss0,len(test_index))\n",
    "                val_loss1 = np.divide(val_loss1,len(test_index))\n",
    "                val_loss_list0.append(val_loss0[0])\n",
    "                val_loss_list1.append(val_loss1[0])\n",
    "                pred_auc0 = roc_auc_score(y_test_all, y_pred_all0)\n",
    "                pred_auc1 = roc_auc_score(y_test_all, y_pred_all1)\n",
    "\n",
    "                print('model0: ','epoch',k,'validation loss',val_loss0,'validation auc',pred_auc0)\n",
    "                print('model1: ','epoch',k,'validation loss',val_loss1,'validation auc',pred_auc1)\n",
    "                with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "                    with open(model_folder1+'cv_results.txt','a') as out1:\n",
    "                        out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss0),'val_auc',str(pred_auc0)])+'\\n')\n",
    "                        out1.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss1),'val_auc',str(pred_auc1)])+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "10000/10000 [==============================] - 40s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "0 2018-08-06 14:18:59.866062\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "1 2018-08-06 14:21:47.144993\n",
      "10000/10000 [==============================] - 39s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "2 2018-08-06 14:24:36.945618\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10000/10000 [==============================] - 37s 4ms/step\n",
      "3 2018-08-06 14:27:23.733606\n",
      "10000/10000 [==============================] - 39s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "4 2018-08-06 14:30:11.634023\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10000/10000 [==============================] - 39s 4ms/step\n",
      "5 2018-08-06 14:32:58.818381\n",
      "10000/10000 [==============================] - 37s 4ms/step\n",
      "10000/10000 [==============================] - 39s 4ms/step\n",
      "6 2018-08-06 14:35:47.832494\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "7 2018-08-06 14:38:35.186283\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "8 2018-08-06 14:41:22.893706\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10000/10000 [==============================] - 39s 4ms/step\n",
      "9 2018-08-06 14:44:12.722193\n",
      "10000/10000 [==============================] - 39s 4ms/step\n",
      "10000/10000 [==============================] - 38s 4ms/step\n",
      "10 2018-08-06 14:47:01.898702\n",
      "model0:  epoch 0 validation loss [ 0.12046457  0.96606364] validation auc 0.838989196327\n",
      "model1:  epoch 0 validation loss [ 0.11922522  0.96620909] validation auc 0.839485140302\n"
     ]
    }
   ],
   "source": [
    "cv_counter = 0\n",
    "for train_index, test_index in kf.split(np.arange(54)):\n",
    "    cv_counter += 1\n",
    "    if cv_counter == 1:\n",
    "        print(test_index)\n",
    "        model0 = load_model('rnn_emb_553/cv_model_1_7.h5')\n",
    "        model1 = load_model('rnn_emb_pooling_553/cv_model_1_7.h5')\n",
    "        \n",
    "        for i in test_index:\n",
    "            loadata = np.load('embedded_x/emb'+str(i)+'.npz')\n",
    "            x=loadata['x']\n",
    "            y=loadata['y']\n",
    "            #x = x[:,time_length:,:]\n",
    "            #x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "            dmg_data = np.load('Data/additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "            x_val_dmg=dmg_data['values']\n",
    "            if i == 53:\n",
    "                dmg_53 = np.zeros((9000,4))\n",
    "                for mm in range(9000):\n",
    "                    dmg_53[mm,:] = x_val_dmg[mm]\n",
    "                x_val_dmg = dmg_53\n",
    "                x = x[:9000]\n",
    "                y = y[:9000]\n",
    "            x_val_dmg[:,2] = abs(x_val_dmg[:,3]-2)\n",
    "            x_val_dmg = x_val_dmg[:,:3]\n",
    "            y_test_all = (y[:,0] if i==test_index[0] else np.append(y_test_all, y[:,0]))\n",
    "            batch_loss0 = model0.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "            batch_loss1 = model1.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "\n",
    "            val_loss0 = np.add(val_loss0,batch_loss0)\n",
    "            val_loss1 = np.add(val_loss1,batch_loss1)\n",
    "\n",
    "            print(i, datetime.now())\n",
    "            y_pred0 = model0.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "            y_pred1 = model1.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "            np.savez_compressed(model_folder0+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred0)\n",
    "            np.savez_compressed(model_folder1+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred1)\n",
    "            y_pred_all0 = (y_pred0 if i==test_index[0] else np.append(y_pred_all0, y_pred0))\n",
    "            y_pred_all1 = (y_pred1 if i==test_index[0] else np.append(y_pred_all1, y_pred1))\n",
    "\n",
    "        val_loss0 = np.divide(val_loss0,len(test_index))\n",
    "        val_loss1 = np.divide(val_loss1,len(test_index))\n",
    "        val_loss_list0.append(val_loss0[0])\n",
    "        val_loss_list1.append(val_loss1[0])\n",
    "        pred_auc0 = roc_auc_score(y_test_all, y_pred_all0)\n",
    "        pred_auc1 = roc_auc_score(y_test_all, y_pred_all1)\n",
    "\n",
    "        print('model0: ','epoch',k,'validation loss',val_loss0,'validation auc',pred_auc0)\n",
    "        print('model1: ','epoch',k,'validation loss',val_loss1,'validation auc',pred_auc1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

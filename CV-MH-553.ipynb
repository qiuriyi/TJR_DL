{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense,Input,GRU,Dropout,concatenate,Permute,Reshape,Lambda,RepeatVector,merge,MaxPooling1D,Embedding,Activation,Conv1D,Flatten\n",
    "import pickle\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import *\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error,roc_curve\n",
    "from keras.backend import eval\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_hidden = 200\n",
    "drop_out = 0.2\n",
    "time_length = 365\n",
    "n_feature = 553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_model():\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "    \n",
    "    gru_h = GRU(n_hidden,activation='tanh', recurrent_activation='sigmoid', \n",
    "                use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                recurrent_initializer='orthogonal', bias_initializer='zeros')(X_input)\n",
    "    gru_h = Dropout(drop_out)(gru_h)\n",
    "    comb = concatenate([gru_h,x_dmgs])\n",
    "    h_t = Dense(1,activation='sigmoid')(comb)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs = h_t)\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "       \n",
    "    preds = Conv1D(3,16,activation='relu')(X_input)\n",
    "    preds = BatchNormalization()(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    preds = MaxPooling1D(pool_size=2)(preds)\n",
    "    preds = BatchNormalization()(preds)\n",
    "    preds = Flatten()(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    comb = concatenate([preds,x_dmgs])\n",
    "    preds = Dense(32,activation='relu')(comb)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    preds = Dense(1,activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs=preds)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pooling_model(ptype):\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "    \n",
    "    gru_h = GRU(n_hidden,activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                recurrent_initializer='orthogonal', bias_initializer='zeros',return_sequences=True)(X_input)\n",
    "    \n",
    "    gru_out = Lambda(lambda x: x[:,-1,:])(gru_h)\n",
    "    gru_out = Reshape((n_hidden,))(gru_out)\n",
    "    \n",
    "    if ptype == 'max':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru])\n",
    "    \n",
    "    elif ptype == 'avg':\n",
    "        avg_gru = Lambda(lambda x: K.mean(x,axis=-2))(gru_h)\n",
    "        avg_gru = Reshape((n_hidden,))(avg_gru)\n",
    "        gru_h = concatenate([gru_out,avg_gru])\n",
    "    \n",
    "    elif ptype == 'min':\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,min_gru])\n",
    "        \n",
    "    elif ptype == 'bpv':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        avg_gru = Lambda(lambda x: K.mean(x,axis=-2))(gru_h)\n",
    "        avg_gru = Reshape((n_hidden,))(avg_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru,avg_gru,min_gru])\n",
    "        \n",
    "    elif ptype == 'maxmin':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru,min_gru])\n",
    "        \n",
    "    #gru_h = concatenate([gru_out,max_gru,avg_gru])\n",
    "    gru_h = Dropout(drop_out)(gru_h)\n",
    "    \n",
    "    comb = concatenate([gru_h,x_dmgs])\n",
    "    h_t = Dense(1,activation='sigmoid')(comb)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs = h_t)\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘rnn_mh_553_one_year/’: File exists\n",
      "mkdir: cannot create directory ‘rnn_mh_pooling_553_one_year/’: File exists\n",
      "[22 23 24 25 26 27 28 29 30 31 32]\n",
      "0 2018-08-07 22:50:27.900928\n",
      "1 2018-08-07 22:50:55.084029\n",
      "2 2018-08-07 22:51:48.127836\n",
      "3 2018-08-07 22:52:16.336751\n",
      "4 2018-08-07 22:52:56.477918\n",
      "5 2018-08-07 22:53:49.190007\n",
      "6 2018-08-07 22:54:16.004137\n",
      "7 2018-08-07 22:54:59.777632\n",
      "8 2018-08-07 22:55:40.944707\n",
      "9 2018-08-07 22:56:07.872005\n",
      "10 2018-08-07 22:57:10.560159\n",
      "11 2018-08-07 22:57:40.844971\n",
      "12 2018-08-07 22:58:14.933369\n",
      "13 2018-08-07 22:59:05.240695\n",
      "14 2018-08-07 22:59:51.573394\n",
      "15 2018-08-07 23:00:42.665730\n",
      "16 2018-08-07 23:01:21.048834\n",
      "17 2018-08-07 23:02:09.151418\n",
      "18 2018-08-07 23:03:07.303459\n",
      "19 2018-08-07 23:03:34.519237\n",
      "20 2018-08-07 23:04:09.974832\n",
      "21 2018-08-07 23:04:56.664673\n"
     ]
    }
   ],
   "source": [
    "bsize = 500\n",
    "max_epoch = 8\n",
    "model_folder0 = 'rnn_mh_553_one_year/'\n",
    "model_folder1 = 'rnn_mh_pooling_553_one_year/'\n",
    "\n",
    "!mkdir $model_folder0\n",
    "!mkdir $model_folder1\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_counter = 0\n",
    "\n",
    "for train_index, test_index in kf.split(np.arange(54)):\n",
    "    cv_counter += 1\n",
    "    if cv_counter > 2:\n",
    "        print(test_index)\n",
    "        with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "            with open(model_folder1+'cv_results.txt','a') as out1:\n",
    "                out0.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "                out1.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "        model0 = get_rnn_model()\n",
    "        model1 = get_pooling_model('bpv')\n",
    "        val_loss_list0,val_loss_list1,val_loss_list2,val_loss_list3 = [],[],[],[]\n",
    "\n",
    "        for k in range(max_epoch):\n",
    "            train_loss0,train_loss1,train_loss2,train_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "            val_loss0,val_loss1,val_loss2,val_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "            y_test_all0,y_test_all1,y_test_all2,y_test_all3 = None,None,None,None\n",
    "            y_pred_all0,y_pred_all1,y_pred_all2,y_pred_all3 = None,None,None,None\n",
    "\n",
    "            for i in train_index:\n",
    "                loadata = np.load('slice_data/0317data'+str(i)+'.npz')\n",
    "                dmg_data = np.load('Data/additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                x_dmg=dmg_data['values']\n",
    "                if i == 53:\n",
    "                    dmg_53 = np.zeros((9000,4))\n",
    "                    for mm in range(9000):\n",
    "                        dmg_53[mm,:] = x_dmg[mm]\n",
    "                    x_dmg = dmg_53\n",
    "                x_dmg[:,2] = abs(x_dmg[:,3]-2)\n",
    "                x_dmg = x_dmg[:,:3]\n",
    "                x = loadata['InputX3D']\n",
    "                y = loadata['Output3D']\n",
    "                x = x[:,time_length:,:]\n",
    "                #x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                for j in range(0,x_dmg.shape[0],bsize):\n",
    "                    batch_loss0 = model0.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                    train_loss0 = np.add(train_loss0,batch_loss0)\n",
    "                    batch_loss1 = model1.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                    train_loss1 = np.add(train_loss1,batch_loss1)\n",
    "                print(i, datetime.now())\n",
    "\n",
    "            model0.save(model_folder0+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "            model1.save(model_folder1+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "\n",
    "            train_loss0 = np.divide(train_loss0,(len(train_index)*10000/bsize))\n",
    "            train_loss1 = np.divide(train_loss1,(len(train_index)*10000/bsize))\n",
    "\n",
    "            print('model0:','epoch',k,'train_loss:',train_loss0)\n",
    "            print('model1:','epoch',k,'train_loss:',train_loss1)\n",
    "            with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "                with open(model_folder1+'cv_results.txt','a') as out1:\n",
    "                    out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss0)])+'\\n')\n",
    "                    out1.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss1)])+'\\n')\n",
    "\n",
    "\n",
    "            if k > max_epoch-2:\n",
    "                for i in test_index:\n",
    "                    loadata = np.load('slice_data/0317data'+str(i)+'.npz')\n",
    "                    x=loadata['InputX3D']\n",
    "                    y=loadata['Output3D']\n",
    "                    x = x[:,time_length:,:]\n",
    "                    #x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                    dmg_data = np.load('Data/additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                    x_val_dmg=dmg_data['values']\n",
    "                    if i == 53:\n",
    "                        dmg_53 = np.zeros((9000,4))\n",
    "                        for mm in range(9000):\n",
    "                            dmg_53[mm,:] = x_val_dmg[mm]\n",
    "                        x_val_dmg = dmg_53\n",
    "                        x = x[:9000]\n",
    "                        y = y[:9000]\n",
    "                    x_val_dmg[:,2] = abs(x_val_dmg[:,3]-2)\n",
    "                    x_val_dmg = x_val_dmg[:,:3]\n",
    "                    y_test_all = (y[:,0] if i==test_index[0] else np.append(y_test_all, y[:,0]))\n",
    "                    batch_loss0 = model0.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                    batch_loss1 = model1.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "\n",
    "                    val_loss0 = np.add(val_loss0,batch_loss0)\n",
    "                    val_loss1 = np.add(val_loss1,batch_loss1)\n",
    "\n",
    "                    print(i, datetime.now())\n",
    "                    y_pred0 = model0.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                    y_pred1 = model1.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                    np.savez_compressed(model_folder0+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred0)\n",
    "                    np.savez_compressed(model_folder1+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred1)\n",
    "                    y_pred_all0 = (y_pred0 if i==test_index[0] else np.append(y_pred_all0, y_pred0))\n",
    "                    y_pred_all1 = (y_pred1 if i==test_index[0] else np.append(y_pred_all1, y_pred1))\n",
    "\n",
    "                val_loss0 = np.divide(val_loss0,len(test_index))\n",
    "                val_loss1 = np.divide(val_loss1,len(test_index))\n",
    "                val_loss_list0.append(val_loss0[0])\n",
    "                val_loss_list1.append(val_loss1[0])\n",
    "                pred_auc0 = roc_auc_score(y_test_all, y_pred_all0)\n",
    "                pred_auc1 = roc_auc_score(y_test_all, y_pred_all1)\n",
    "\n",
    "                print('model0: ','epoch',k,'validation loss',val_loss0,'validation auc',pred_auc0)\n",
    "                print('model1: ','epoch',k,'validation loss',val_loss1,'validation auc',pred_auc1)\n",
    "                with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "                    with open(model_folder1+'cv_results.txt','a') as out1:\n",
    "                        out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss0),'val_auc',str(pred_auc0)])+'\\n')\n",
    "                        out1.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss1),'val_auc',str(pred_auc1)])+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in kf.split(np.arange(54)):\n",
    "    cv_counter += 1\n",
    "    if cv_counter <= 2:\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

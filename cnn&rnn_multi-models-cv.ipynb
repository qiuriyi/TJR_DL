{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense,Input,GRU,Dropout,concatenate,Permute,Reshape,Lambda,RepeatVector,merge,MaxPooling1D,Embedding,Activation,Conv1D,Flatten\n",
    "import pickle\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from keras.initializers import RandomUniform\n",
    "from keras.optimizers import *\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error,roc_curve\n",
    "from keras.backend import eval\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "n_hidden = 200\n",
    "drop_out = 0.2\n",
    "time_length = 365\n",
    "n_feature = 2070\n",
    "n_emb = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rnn_model():\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "    \n",
    "    gru_h = GRU(n_hidden,activation='tanh', recurrent_activation='sigmoid', \n",
    "                use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                recurrent_initializer='orthogonal', bias_initializer='zeros')(X_input)\n",
    "    gru_h = Dropout(drop_out)(gru_h)\n",
    "    comb = concatenate([gru_h,x_dmgs])\n",
    "    h_t = Dense(1,activation='sigmoid')(comb)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs = h_t)\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cnn_model():\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "       \n",
    "    preds = Conv1D(3,16,activation='relu')(X_input)\n",
    "    preds = BatchNormalization()(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    preds = MaxPooling1D(pool_size=2)(preds)\n",
    "    preds = BatchNormalization()(preds)\n",
    "    preds = Flatten()(preds)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    comb = concatenate([preds,x_dmgs])\n",
    "    preds = Dense(32,activation='relu')(comb)\n",
    "    preds = Dropout(0.5)(preds)\n",
    "    preds = Dense(1,activation='sigmoid')(preds)\n",
    "\n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs=preds)\n",
    "\n",
    "    # initiate RMSprop optimizer\n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    # Let's train the model using RMSprop\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pooling_model(ptype):\n",
    "    X_input = Input(shape=(time_length,n_feature), name='x_main', dtype='float32')\n",
    "    x_dmgs = Input(shape=(3,), name='x_dmgs', dtype='float32')\n",
    "    \n",
    "    gru_h = GRU(n_hidden,activation='tanh', recurrent_activation='sigmoid', use_bias=True, kernel_initializer='glorot_uniform', \n",
    "                recurrent_initializer='orthogonal', bias_initializer='zeros',return_sequences=True)(X_input)\n",
    "    \n",
    "    gru_out = Lambda(lambda x: x[:,-1,:])(gru_h)\n",
    "    gru_out = Reshape((n_hidden,))(gru_out)\n",
    "    \n",
    "    if ptype == 'max':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru])\n",
    "    \n",
    "    elif ptype == 'avg':\n",
    "        avg_gru = Lambda(lambda x: K.mean(x,axis=-2))(gru_h)\n",
    "        avg_gru = Reshape((n_hidden,))(avg_gru)\n",
    "        gru_h = concatenate([gru_out,avg_gru])\n",
    "    \n",
    "    elif ptype == 'min':\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,min_gru])\n",
    "        \n",
    "    elif ptype == 'bpv':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        avg_gru = Lambda(lambda x: K.mean(x,axis=-2))(gru_h)\n",
    "        avg_gru = Reshape((n_hidden,))(avg_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru,avg_gru,min_gru])\n",
    "        \n",
    "    elif ptype == 'maxmin':\n",
    "        max_gru = MaxPooling1D(pool_size=time_length)(gru_h)\n",
    "        max_gru = Reshape((n_hidden,))(max_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(gru_h)\n",
    "        min_gru = MaxPooling1D(pool_size=time_length)(min_gru)\n",
    "        min_gru = Lambda(lambda x: -x)(min_gru)\n",
    "        min_gru = Reshape((n_hidden,))(min_gru)\n",
    "        gru_h = concatenate([gru_out,max_gru,min_gru])\n",
    "        \n",
    "    #gru_h = concatenate([gru_out,max_gru,avg_gru])\n",
    "    gru_h = Dropout(drop_out)(gru_h)\n",
    "    \n",
    "    comb = concatenate([gru_h,x_dmgs])\n",
    "    h_t = Dense(1,activation='sigmoid')(comb)\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=[X_input, x_dmgs], outputs = h_t)\n",
    "    \n",
    "    opt = keras.optimizers.adam(lr=learning_rate)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘one_year_prediction/cnn/’: File exists\n",
      "mkdir: cannot create directory ‘one_year_prediction/rnn-mh/’: File exists\n",
      "mkdir: cannot create directory ‘one_year_prediction/rnn-mh-bpv/’: File exists\n",
      "mkdir: cannot create directory ‘one_year_prediction/rnn-mh-maxmin/’: File exists\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "11 2018-08-01 01:03:11.965982\n",
      "12 2018-08-01 01:09:34.030762\n"
     ]
    }
   ],
   "source": [
    "bsize = 250\n",
    "max_epoch = 6\n",
    "model_folder0 = 'one_year_prediction/cnn/'\n",
    "model_folder1 = 'one_year_prediction/rnn-mh/'\n",
    "model_folder2 = 'one_year_prediction/rnn-mh-bpv/'\n",
    "model_folder3 = 'one_year_prediction/rnn-mh-maxmin/'\n",
    "!mkdir $model_folder0\n",
    "!mkdir $model_folder1\n",
    "!mkdir $model_folder2\n",
    "!mkdir $model_folder3\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_counter = 0\n",
    "with open(model_folder0+'cv_results.txt','w') as out0:\n",
    "    with open(model_folder1+'cv_results.txt','w') as out1:\n",
    "        with open(model_folder2+'cv_results.txt','w') as out2:\n",
    "            with open(model_folder3+'cv_results.txt','w') as out3:\n",
    "                for train_index, test_index in kf.split(np.arange(54)):\n",
    "                    cv_counter += 1\n",
    "                    if cv_counter > 2:\n",
    "                        print(test_index)\n",
    "                        out0.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "                        out1.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "                        out2.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "                        out3.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "                        model0 = get_cnn_model()\n",
    "                        model1 = get_rnn_model()\n",
    "                        model2 = get_pooling_model(ptype='bpv')\n",
    "                        model3 = get_pooling_model(ptype='maxmin')\n",
    "                        val_loss_list0,val_loss_list1,val_loss_list2,val_loss_list3 = [],[],[],[]\n",
    "\n",
    "                        for k in range(max_epoch):\n",
    "                            train_loss0,train_loss1,train_loss2,train_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "                            val_loss0,val_loss1,val_loss2,val_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "                            y_test_all0,y_test_all1,y_test_all2,y_test_all3 = None,None,None,None\n",
    "                            y_pred_all0,y_pred_all1,y_pred_all2,y_pred_all3 = None,None,None,None\n",
    "\n",
    "                            for i in train_index:\n",
    "                                loadata = np.load('0702data'+str(i)+'.npz')\n",
    "                                dmg_data = np.load('additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                                x_dmg=dmg_data['values']\n",
    "                                if i == 53:\n",
    "                                    dmg_53 = np.zeros((9000,4))\n",
    "                                    for mm in range(9000):\n",
    "                                        dmg_53[mm,:] = x_dmg[mm]\n",
    "                                    x_dmg = dmg_53\n",
    "                                x_dmg[:,2] = abs(x_dmg[:,3]-2)\n",
    "                                x_dmg = x_dmg[:,:3]\n",
    "                                x = loadata['InputX3D']\n",
    "                                y = loadata['Output3D']\n",
    "                                ### use this when running the model with only one year data\n",
    "                                ### x = x[:,365:,:]\n",
    "                                x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                                for j in range(0,x_dmg.shape[0],bsize):\n",
    "                                    batch_loss0 = model0.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                                    train_loss0 = np.add(train_loss0,batch_loss0)\n",
    "                                    batch_loss1 = model1.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                                    train_loss1 = np.add(train_loss1,batch_loss1)\n",
    "                                    batch_loss2 = model2.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                                    train_loss2 = np.add(train_loss2,batch_loss2)\n",
    "                                    batch_loss3 = model3.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                                    train_loss3 = np.add(train_loss3,batch_loss3)\n",
    "                                print(i, datetime.now())\n",
    "\n",
    "                            model0.save(model_folder0+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "                            model1.save(model_folder1+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "                            model2.save(model_folder2+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "                            model3.save(model_folder3+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "                            train_loss0 = np.divide(train_loss0,(len(train_index)*10000/bsize))\n",
    "                            train_loss1 = np.divide(train_loss1,(len(train_index)*10000/bsize))\n",
    "                            train_loss2 = np.divide(train_loss2,(len(train_index)*10000/bsize))\n",
    "                            train_loss3 = np.divide(train_loss3,(len(train_index)*10000/bsize))\n",
    "                            print('model0:','epoch',k,'train_loss:',train_loss0)\n",
    "                            print('model1:','epoch',k,'train_loss:',train_loss1)\n",
    "                            print('model2:','epoch',k,'train_loss:',train_loss2)\n",
    "                            print('model3:','epoch',k,'train_loss:',train_loss3)\n",
    "                            out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss0)])+'\\n')\n",
    "                            out1.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss1)])+'\\n')\n",
    "                            out2.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss2)])+'\\n')\n",
    "                            out3.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss3)])+'\\n')\n",
    "\n",
    "                            if k > 1:\n",
    "                                for i in test_index:\n",
    "                                    loadata = np.load('0702data'+str(i)+'.npz')\n",
    "                                    x=loadata['InputX3D']\n",
    "                                    y=loadata['Output3D']\n",
    "                                    ### use this when running the model with only one year data\n",
    "                                    ### x = x[:,time_length:,:]\n",
    "                                    x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                                    dmg_data = np.load('additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                                    x_val_dmg=dmg_data['values']\n",
    "                                    if i == 53:\n",
    "                                        dmg_53 = np.zeros((9000,4))\n",
    "                                        for mm in range(9000):\n",
    "                                            dmg_53[mm,:] = x_val_dmg[mm]\n",
    "                                        x_val_dmg = dmg_53\n",
    "                                        x = x[:9000]\n",
    "                                        y = y[:9000]\n",
    "                                    x_val_dmg[:,2] = abs(x_val_dmg[:,3]-2)\n",
    "                                    x_val_dmg = x_val_dmg[:,:3]\n",
    "                                    y_test_all = (y[:,0] if i==test_index[0] else np.append(y_test_all, y[:,0]))\n",
    "                                    batch_loss0 = model0.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                                    batch_loss1 = model1.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                                    batch_loss2 = model2.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                                    batch_loss3 = model3.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                                    val_loss0 = np.add(val_loss0,batch_loss0)\n",
    "                                    val_loss1 = np.add(val_loss1,batch_loss1)\n",
    "                                    val_loss2 = np.add(val_loss2,batch_loss2)\n",
    "                                    val_loss3 = np.add(val_loss3,batch_loss3)\n",
    "                                    print(i, datetime.now())\n",
    "                                    y_pred0 = model0.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                                    np.savez_compressed(model_folder0+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred0)\n",
    "                                    y_pred_all0 = (y_pred0 if i==test_index[0] else np.append(y_pred_all0, y_pred0))\n",
    "                                    y_pred1 = model1.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                                    np.savez_compressed(model_folder1+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred1)\n",
    "                                    y_pred_all1 = (y_pred1 if i==test_index[0] else np.append(y_pred_all1, y_pred1))\n",
    "                                    y_pred2 = model2.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                                    np.savez_compressed(model_folder2+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred2)\n",
    "                                    y_pred_all2 = (y_pred2 if i==test_index[0] else np.append(y_pred_all2, y_pred2))\n",
    "                                    y_pred3 = model3.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                                    np.savez_compressed(model_folder3+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred3)\n",
    "                                    y_pred_all3 = (y_pred3 if i==test_index[0] else np.append(y_pred_all3, y_pred3))\n",
    "\n",
    "                                val_loss0 = np.divide(val_loss0,len(test_index))\n",
    "                                val_loss_list0.append(val_loss0[0])\n",
    "                                pred_auc0 = roc_auc_score(y_test_all, y_pred_all0)\n",
    "                                val_loss1 = np.divide(val_loss1,len(test_index))\n",
    "                                val_loss_list1.append(val_loss1[0])\n",
    "                                pred_auc1 = roc_auc_score(y_test_all, y_pred_all1)\n",
    "                                val_loss2 = np.divide(val_loss2,len(test_index))\n",
    "                                val_loss_list2.append(val_loss2[0])\n",
    "                                pred_auc2 = roc_auc_score(y_test_all, y_pred_all2)\n",
    "                                val_loss3 = np.divide(val_loss3,len(test_index))\n",
    "                                val_loss_list3.append(val_loss3[0])\n",
    "                                pred_auc3 = roc_auc_score(y_test_all, y_pred_all3)\n",
    "                                print('model0: ','epoch',k,'validation loss',val_loss0,'validation auc',pred_auc0)\n",
    "                                print('model1: ','epoch',k,'validation loss',val_loss1,'validation auc',pred_auc1)\n",
    "                                print('model2: ','epoch',k,'validation loss',val_loss2,'validation auc',pred_auc2)\n",
    "                                print('model3: ','epoch',k,'validation loss',val_loss3,'validation auc',pred_auc3)\n",
    "                                out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss0),'val_auc',str(pred_auc0)])+'\\n')\n",
    "                                out1.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss1),'val_auc',str(pred_auc1)])+'\\n')\n",
    "                                out2.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss2),'val_auc',str(pred_auc2)])+'\\n')\n",
    "                                out3.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss3),'val_auc',str(pred_auc3)])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘one_year_prediction/rnn_mh/’: File exists\n",
      "[33 34 35 36 37 38 39 40 41 42 43]\n",
      "0 2018-08-04 11:18:44.979755\n",
      "1 2018-08-04 11:19:43.327630\n",
      "2 2018-08-04 11:20:42.215536\n",
      "3 2018-08-04 11:21:41.235919\n",
      "4 2018-08-04 11:22:39.604062\n",
      "5 2018-08-04 11:23:38.311938\n",
      "6 2018-08-04 11:24:36.784239\n",
      "7 2018-08-04 11:25:35.698139\n",
      "8 2018-08-04 11:26:34.257972\n",
      "9 2018-08-04 11:27:33.899851\n",
      "10 2018-08-04 11:28:32.824257\n",
      "11 2018-08-04 11:29:31.962498\n",
      "12 2018-08-04 11:30:30.906894\n",
      "13 2018-08-04 11:31:30.081898\n",
      "14 2018-08-04 11:32:28.658309\n",
      "15 2018-08-04 11:33:27.274018\n",
      "16 2018-08-04 11:34:26.301747\n",
      "17 2018-08-04 11:35:24.832549\n",
      "18 2018-08-04 11:36:23.852417\n",
      "19 2018-08-04 11:37:23.016510\n",
      "20 2018-08-04 11:38:22.006359\n",
      "21 2018-08-04 11:39:20.594635\n",
      "22 2018-08-04 11:40:19.401763\n",
      "23 2018-08-04 11:41:17.606612\n",
      "24 2018-08-04 11:42:16.683735\n",
      "25 2018-08-04 11:43:15.373837\n",
      "26 2018-08-04 11:44:14.242277\n",
      "27 2018-08-04 11:45:13.084487\n",
      "28 2018-08-04 11:46:11.431142\n",
      "29 2018-08-04 11:47:10.676420\n",
      "30 2018-08-04 11:48:09.288842\n",
      "31 2018-08-04 11:49:08.444007\n",
      "32 2018-08-04 11:50:06.901189\n",
      "44 2018-08-04 11:51:05.086856\n",
      "45 2018-08-04 11:52:03.756397\n",
      "46 2018-08-04 11:53:02.031458\n",
      "47 2018-08-04 11:54:01.883165\n",
      "48 2018-08-04 11:55:00.415472\n",
      "49 2018-08-04 11:55:59.202445\n",
      "50 2018-08-04 11:56:58.728690\n",
      "51 2018-08-04 11:57:58.109710\n",
      "52 2018-08-04 11:58:56.894414\n",
      "53 2018-08-04 11:59:54.336333\n",
      "model0: epoch 0 train_loss: [ 0.14516837  0.95665349]\n",
      "0 2018-08-04 12:00:53.168386\n",
      "1 2018-08-04 12:01:51.553878\n",
      "2 2018-08-04 12:02:51.069473\n",
      "3 2018-08-04 12:03:50.147705\n",
      "4 2018-08-04 12:04:48.926666\n",
      "5 2018-08-04 12:05:47.628844\n",
      "6 2018-08-04 12:06:46.458732\n",
      "7 2018-08-04 12:07:46.126167\n",
      "8 2018-08-04 12:08:44.564707\n",
      "9 2018-08-04 12:09:43.915894\n",
      "10 2018-08-04 12:10:42.533472\n",
      "11 2018-08-04 12:11:41.350588\n",
      "12 2018-08-04 12:12:40.587416\n",
      "13 2018-08-04 12:13:39.609574\n",
      "14 2018-08-04 12:14:38.532347\n",
      "15 2018-08-04 12:15:37.799079\n",
      "16 2018-08-04 12:16:36.038085\n",
      "17 2018-08-04 12:17:35.130994\n",
      "18 2018-08-04 12:18:33.785255\n",
      "19 2018-08-04 12:19:32.256481\n",
      "20 2018-08-04 12:20:31.365458\n",
      "21 2018-08-04 12:21:30.660783\n",
      "22 2018-08-04 12:22:29.902118\n",
      "23 2018-08-04 12:23:28.386349\n",
      "24 2018-08-04 12:24:27.615147\n",
      "25 2018-08-04 12:25:26.605617\n",
      "26 2018-08-04 12:26:25.738715\n",
      "27 2018-08-04 12:27:24.480035\n",
      "28 2018-08-04 12:28:23.576730\n",
      "29 2018-08-04 12:29:21.957811\n",
      "30 2018-08-04 12:30:20.759699\n",
      "31 2018-08-04 12:31:19.232984\n",
      "32 2018-08-04 12:32:17.562671\n",
      "44 2018-08-04 12:33:17.214093\n",
      "45 2018-08-04 12:34:16.190273\n",
      "46 2018-08-04 12:35:14.996841\n",
      "47 2018-08-04 12:36:13.866479\n",
      "48 2018-08-04 12:37:12.874259\n",
      "49 2018-08-04 12:38:11.656396\n",
      "50 2018-08-04 12:39:10.900210\n"
     ]
    }
   ],
   "source": [
    "bsize = 250\n",
    "max_epoch = 4\n",
    "model_folder0 = 'one_year_prediction/rnn_mh/'\n",
    "\n",
    "!mkdir $model_folder0\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_counter = 0\n",
    "with open(model_folder0+'cv_results.txt','a') as out0:\n",
    "    for train_index, test_index in kf.split(np.arange(54)):\n",
    "        cv_counter += 1\n",
    "        if cv_counter > 3:\n",
    "            print(test_index)\n",
    "            out0.write(\"##\".join(['cv order',str(cv_counter),'leave out index',str(test_index)])+'\\n\\n')\n",
    "            model0 = get_rnn_model()\n",
    "            val_loss_list0,val_loss_list1,val_loss_list2,val_loss_list3 = [],[],[],[]\n",
    "\n",
    "            for k in range(max_epoch):\n",
    "                train_loss0,train_loss1,train_loss2,train_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "                val_loss0,val_loss1,val_loss2,val_loss3 = np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0]),np.array([0.0,0.0])\n",
    "                y_test_all0,y_test_all1,y_test_all2,y_test_all3 = None,None,None,None\n",
    "                y_pred_all0,y_pred_all1,y_pred_all2,y_pred_all3 = None,None,None,None\n",
    "\n",
    "                for i in train_index:\n",
    "                    loadata = np.load('0702data'+str(i)+'.npz')\n",
    "                    dmg_data = np.load('additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                    x_dmg=dmg_data['values']\n",
    "                    if i == 53:\n",
    "                        dmg_53 = np.zeros((9000,4))\n",
    "                        for mm in range(9000):\n",
    "                            dmg_53[mm,:] = x_dmg[mm]\n",
    "                        x_dmg = dmg_53\n",
    "                    x_dmg[:,2] = abs(x_dmg[:,3]-2)\n",
    "                    x_dmg = x_dmg[:,:3]\n",
    "                    x = loadata['InputX3D']\n",
    "                    y = loadata['Output3D']\n",
    "                    ### use this when running the model with only one year data\n",
    "                    ### x = x[:,time_length:,:]\n",
    "                    x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                    for j in range(0,x_dmg.shape[0],bsize):\n",
    "                        batch_loss0 = model0.train_on_batch({'x_main':x[j:j+bsize,:,:],'x_dmgs':x_dmg[j:j+bsize]}, y[j:j+bsize,0])\n",
    "                        train_loss0 = np.add(train_loss0,batch_loss0)\n",
    "                    print(i, datetime.now())\n",
    "\n",
    "                model0.save(model_folder0+'cv_model_'+str(cv_counter)+'_'+str(k)+'.h5')\n",
    "                \n",
    "                train_loss0 = np.divide(train_loss0,(len(train_index)*10000/bsize))\n",
    "                \n",
    "                print('model0:','epoch',k,'train_loss:',train_loss0)\n",
    "                \n",
    "                out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'train loss',str(train_loss0)])+'\\n')\n",
    "                \n",
    "\n",
    "                if k > 1:\n",
    "                    for i in test_index:\n",
    "                        loadata = np.load('0702data'+str(i)+'.npz')\n",
    "                        x=loadata['InputX3D']\n",
    "                        y=loadata['Output3D']\n",
    "                        ### use this when running the model with only one year data\n",
    "                        ### x = x[:,time_length:,:]\n",
    "                        x=np.concatenate((x[:,:,:505],x[:,:,539:]),axis=2)\n",
    "                        dmg_data = np.load('additional_fields/0317data'+str(i)+'_additionalFields.npz')\n",
    "                        x_val_dmg=dmg_data['values']\n",
    "                        if i == 53:\n",
    "                            dmg_53 = np.zeros((9000,4))\n",
    "                            for mm in range(9000):\n",
    "                                dmg_53[mm,:] = x_val_dmg[mm]\n",
    "                            x_val_dmg = dmg_53\n",
    "                            x = x[:9000]\n",
    "                            y = y[:9000]\n",
    "                        x_val_dmg[:,2] = abs(x_val_dmg[:,3]-2)\n",
    "                        x_val_dmg = x_val_dmg[:,:3]\n",
    "                        y_test_all = (y[:,0] if i==test_index[0] else np.append(y_test_all, y[:,0]))\n",
    "                        batch_loss0 = model0.evaluate({'x_main':x,'x_dmgs':x_val_dmg},y[:,0])\n",
    "                        \n",
    "                        val_loss0 = np.add(val_loss0,batch_loss0)\n",
    "                       \n",
    "                        print(i, datetime.now())\n",
    "                        y_pred0 = model0.predict({'x_main':x,'x_dmgs':x_val_dmg})\n",
    "                        np.savez_compressed(model_folder0+'pred_y_'+str(cv_counter)+'_'+str(k)+'_'+str(i),y_pred=y_pred0)\n",
    "                        y_pred_all0 = (y_pred0 if i==test_index[0] else np.append(y_pred_all0, y_pred0))\n",
    "                       \n",
    "                    val_loss0 = np.divide(val_loss0,len(test_index))\n",
    "                    val_loss_list0.append(val_loss0[0])\n",
    "                    pred_auc0 = roc_auc_score(y_test_all, y_pred_all0)\n",
    "                    \n",
    "                    print('model0: ','epoch',k,'validation loss',val_loss0,'validation auc',pred_auc0)\n",
    "                   \n",
    "                    out0.write(\"\\t\".join([str(datetime.now()),'epoch',str(k),'validation loss',str(val_loss0),'val_auc',str(pred_auc0)])+'\\n')\n",
    "                    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

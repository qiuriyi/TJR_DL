{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, concatenate, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import urllib.request\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pkl file\n",
    "def load_obj(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "dictionary = load_obj('dictionary.pkl')\n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-06-11 22:29:06.002176\n",
      "1 2018-06-11 22:33:46.630312\n",
      "2 2018-06-11 22:38:38.123163\n",
      "3 2018-06-11 22:43:35.201380\n",
      "4 2018-06-11 22:48:33.851698\n",
      "5 2018-06-11 22:53:30.058311\n",
      "6 2018-06-11 22:58:50.262898\n",
      "7 2018-06-11 23:04:05.637833\n",
      "8 2018-06-11 23:09:34.540393\n",
      "9 2018-06-11 23:15:15.046381\n",
      "10 2018-06-11 23:21:14.058447\n",
      "11 2018-06-11 23:27:05.498806\n",
      "12 2018-06-11 23:32:44.101493\n",
      "13 2018-06-11 23:39:06.088560\n",
      "14 2018-06-11 23:45:17.341725\n",
      "15 2018-06-11 23:51:57.090448\n",
      "16 2018-06-11 23:58:43.435506\n",
      "17 2018-06-12 00:05:51.585762\n",
      "18 2018-06-12 00:13:03.665627\n",
      "19 2018-06-12 00:20:59.391889\n",
      "20 2018-06-12 00:28:12.679750\n",
      "21 2018-06-12 00:35:51.798155\n",
      "22 2018-06-12 00:43:41.048729\n",
      "23 2018-06-12 00:51:01.780385\n",
      "24 2018-06-12 00:59:12.913222\n",
      "25 2018-06-12 01:07:12.122198\n",
      "26 2018-06-12 01:15:27.835303\n",
      "27 2018-06-12 01:24:34.573588\n",
      "28 2018-06-12 01:32:22.534787\n",
      "29 2018-06-12 01:41:09.960389\n",
      "30 2018-06-12 01:50:11.899594\n",
      "31 2018-06-12 01:59:09.960091\n",
      "32 2018-06-12 02:07:36.834486\n",
      "33 2018-06-12 02:16:53.403230\n",
      "34 2018-06-12 02:26:56.466211\n",
      "35 2018-06-12 02:36:10.446119\n",
      "36 2018-06-12 02:45:37.930712\n",
      "37 2018-06-12 02:53:57.896238\n",
      "38 2018-06-12 03:02:01.328007\n",
      "39 2018-06-12 03:09:49.938640\n"
     ]
    }
   ],
   "source": [
    "time_window = 14\n",
    "window_size = 100\n",
    "window = np.zeros(window_size,dtype=int)\n",
    "sampling_table = sequence.make_sampling_table(554,sampling_factor = 1e-05)\n",
    "word_target, labels, word_context = [], [], []\n",
    "\n",
    "for l in range(40):\n",
    "    sentences = []\n",
    "    print(l,datetime.datetime.now())\n",
    "    loadata = np.load('slice_data/0317data'+str(l)+'.npz')\n",
    "    x=loadata['InputX3D']\n",
    "    for p in range(len(x)):\n",
    "        \n",
    "        patient_sentences = []\n",
    "        for i in range(len(x[p]-time_window)):\n",
    "            xx = np.multiply(np.minimum(np.sum(x[p,i:i+time_window,],0),1),np.arange(553)+1)\n",
    "            if np.sum(xx) > 0:\n",
    "                patient_sentences.append(xx[xx != 0].tolist())\n",
    "        k = sorted(patient_sentences)\n",
    "        for key in list(k for k,_ in itertools.groupby(k)):\n",
    "            sentences.extend(key)\n",
    "            sentences.extend(window)\n",
    "    \n",
    "    local_couples, local_labels = skipgrams(sentences, 553, window_size=window_size, sampling_table=sampling_table)\n",
    "    local_word_target, local_word_context = zip(*local_couples)\n",
    "    labels.extend(local_labels)\n",
    "    word_target.extend(local_word_target)\n",
    "    word_context.extend(local_word_context)\n",
    "    \n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273897072"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-11 20:04:12.573589\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "sampling_table = sequence.make_sampling_table(554,sampling_factor = 1e-05)\n",
    "couples, labels = skipgrams(sentences, 553, window_size=window_size, sampling_table=sampling_table)\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_target, word_context = zip(*couples)\n",
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "vocab_size = 553"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"de...)`\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"do...)`\n"
     ]
    }
   ],
   "source": [
    "vector_dim = 100\n",
    "epochs = 5000000\n",
    "\n",
    "# create some input variables\n",
    "input_target = Input((1,))\n",
    "input_context = Input((1,))\n",
    "\n",
    "embedding = Embedding(553, vector_dim, input_length=1, name='embedding')\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)\n",
    "\n",
    "# setup a cosine similarity operation which will be output in a secondary model\n",
    "similarity = dot([target, context], normalize=True, axes=0)\n",
    "\n",
    "# now perform the dot product operation to get a similarity measure\n",
    "dot_product = dot([target, context], normalize=False, axes=1)\n",
    "dot_product = Reshape((1,))(dot_product)\n",
    "# add the sigmoid output layer\n",
    "output = Dense(1, activation='sigmoid')(dot_product)\n",
    "# create the primary training model\n",
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "# create a secondary validation model to run our similarity checks during training\n",
    "validation_model = Model(input=[input_target, input_context], output=similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reverse_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reverse_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = validation_model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss=0.7138327956199646\n",
      "2018-06-12 03:22:36.791663\n",
      "Iteration 10000, loss=0.5671801567077637\n",
      "Iteration 20000, loss=3.672576665878296\n",
      "Iteration 30000, loss=0.0007565499399788678\n",
      "Iteration 40000, loss=2.4368042945861816\n",
      "Iteration 50000, loss=0.20076459646224976\n",
      "Iteration 60000, loss=0.04349581152200699\n",
      "Iteration 70000, loss=0.15330541133880615\n",
      "Iteration 80000, loss=0.8756828308105469\n",
      "Iteration 90000, loss=14.332947731018066\n",
      "Iteration 100000, loss=0.1488679051399231\n",
      "2018-06-12 03:25:59.701746\n",
      "Iteration 110000, loss=1.4145479202270508\n",
      "Iteration 120000, loss=1.4139857292175293\n",
      "Iteration 130000, loss=0.2694445252418518\n",
      "Iteration 140000, loss=0.2684411406517029\n",
      "Iteration 150000, loss=0.10923115164041519\n",
      "Iteration 160000, loss=0.00037677225191146135\n",
      "Iteration 170000, loss=0.0016777870478108525\n",
      "Iteration 180000, loss=0.26924002170562744\n",
      "Iteration 190000, loss=0.1768321394920349\n",
      "Iteration 200000, loss=0.3044373393058777\n",
      "2018-06-12 03:29:20.094175\n",
      "Iteration 210000, loss=0.2853676676750183\n",
      "Iteration 220000, loss=0.04448915645480156\n",
      "Iteration 230000, loss=0.3023352324962616\n",
      "Iteration 240000, loss=6.556533207913162e-06\n",
      "Iteration 250000, loss=0.13176876306533813\n",
      "Iteration 260000, loss=0.0009308018488809466\n",
      "Iteration 270000, loss=1.192093321833454e-07\n",
      "Iteration 280000, loss=0.14692699909210205\n",
      "Iteration 290000, loss=0.11739139258861542\n",
      "Iteration 300000, loss=0.09716585278511047\n",
      "2018-06-12 03:32:49.630591\n",
      "Iteration 310000, loss=0.0538339801132679\n",
      "Iteration 320000, loss=0.4194835126399994\n",
      "Iteration 330000, loss=2.0265786588424817e-05\n",
      "Iteration 340000, loss=0.2790614068508148\n",
      "Iteration 350000, loss=0.0002938344841822982\n",
      "Iteration 360000, loss=0.14824490249156952\n",
      "Iteration 370000, loss=0.7341935038566589\n",
      "Iteration 380000, loss=0.22978119552135468\n",
      "Iteration 390000, loss=1.1072874069213867\n",
      "Iteration 400000, loss=0.00025543858646415174\n",
      "2018-06-12 03:36:05.369820\n",
      "Iteration 410000, loss=10.238603591918945\n",
      "Iteration 420000, loss=1.192093321833454e-07\n",
      "Iteration 430000, loss=0.2895008623600006\n",
      "Iteration 440000, loss=1.90735113392293e-06\n",
      "Iteration 450000, loss=0.18436719477176666\n",
      "Iteration 460000, loss=0.1619465947151184\n",
      "Iteration 470000, loss=10.434538841247559\n",
      "Iteration 480000, loss=1.192093321833454e-07\n",
      "Iteration 490000, loss=1.769760251045227\n",
      "Iteration 500000, loss=0.29082098603248596\n",
      "2018-06-12 03:39:29.488474\n",
      "Iteration 510000, loss=1.192093321833454e-07\n",
      "Iteration 520000, loss=0.001938850386068225\n",
      "Iteration 530000, loss=15.618334770202637\n",
      "Iteration 540000, loss=15.942384719848633\n",
      "Iteration 550000, loss=3.802846549660899e-05\n",
      "Iteration 560000, loss=0.019785786047577858\n",
      "Iteration 570000, loss=0.02173563465476036\n",
      "Iteration 580000, loss=0.13308066129684448\n",
      "Iteration 590000, loss=1.192093321833454e-07\n",
      "Iteration 600000, loss=9.387930870056152\n",
      "2018-06-12 03:42:59.134025\n",
      "Iteration 610000, loss=1.7129578590393066\n",
      "Iteration 620000, loss=0.008137641474604607\n",
      "Iteration 630000, loss=0.10022479295730591\n",
      "Iteration 640000, loss=0.00045906114974059165\n",
      "Iteration 650000, loss=4.30027961730957\n",
      "Iteration 660000, loss=0.7181764245033264\n",
      "Iteration 670000, loss=2.896826663345564e-05\n",
      "Iteration 680000, loss=0.018157143145799637\n",
      "Iteration 690000, loss=0.07888559252023697\n",
      "Iteration 700000, loss=0.0001266083272639662\n",
      "2018-06-12 03:46:13.274112\n",
      "Iteration 710000, loss=3.552499401848763e-05\n",
      "Iteration 720000, loss=0.14588560163974762\n",
      "Iteration 730000, loss=1.192093321833454e-07\n",
      "Iteration 740000, loss=0.5239816308021545\n",
      "Iteration 750000, loss=0.003757672617211938\n",
      "Iteration 760000, loss=0.010708577930927277\n",
      "Iteration 770000, loss=1.0194504261016846\n",
      "Iteration 780000, loss=0.18798907101154327\n",
      "Iteration 790000, loss=0.5942980647087097\n",
      "Iteration 800000, loss=0.27352234721183777\n",
      "2018-06-12 03:49:41.386064\n",
      "Iteration 810000, loss=1.8200111389160156\n",
      "Iteration 820000, loss=0.08603422343730927\n",
      "Iteration 830000, loss=0.011049395427107811\n",
      "Iteration 840000, loss=0.048044610768556595\n",
      "Iteration 850000, loss=0.009371346794068813\n",
      "Iteration 860000, loss=0.0047640325501561165\n",
      "Iteration 870000, loss=0.04449966922402382\n",
      "Iteration 880000, loss=0.012685067020356655\n",
      "Iteration 890000, loss=0.6799148917198181\n",
      "Iteration 900000, loss=0.02039661444723606\n",
      "2018-06-12 03:53:07.606357\n",
      "Iteration 910000, loss=0.018565401434898376\n",
      "Iteration 920000, loss=1.192093321833454e-07\n",
      "Iteration 930000, loss=0.013006369583308697\n",
      "Iteration 940000, loss=0.45078518986701965\n",
      "Iteration 950000, loss=4.583181858062744\n",
      "Iteration 960000, loss=0.13196811079978943\n",
      "Iteration 970000, loss=1.192093321833454e-07\n",
      "Iteration 980000, loss=1.192093321833454e-07\n",
      "Iteration 990000, loss=0.20302404463291168\n",
      "Iteration 1000000, loss=0.00021358633239287883\n",
      "2018-06-12 03:56:25.460190\n",
      "Iteration 1010000, loss=16.11809539794922\n",
      "Iteration 1020000, loss=3.20194411277771\n",
      "Iteration 1030000, loss=0.23580193519592285\n",
      "Iteration 1040000, loss=0.2618245780467987\n",
      "Iteration 1050000, loss=6.234841566765681e-05\n",
      "Iteration 1060000, loss=2.6058731079101562\n",
      "Iteration 1070000, loss=3.585913896560669\n",
      "Iteration 1080000, loss=0.29881444573402405\n",
      "Iteration 1090000, loss=0.0017672183457762003\n",
      "Iteration 1100000, loss=0.30619335174560547\n",
      "2018-06-12 03:59:55.632121\n",
      "Iteration 1110000, loss=1.6245373487472534\n",
      "Iteration 1120000, loss=0.058559857308864594\n",
      "Iteration 1130000, loss=1.192093321833454e-07\n",
      "Iteration 1140000, loss=5.149973731022328e-05\n",
      "Iteration 1150000, loss=0.0078063709661364555\n",
      "Iteration 1160000, loss=0.3716890215873718\n",
      "Iteration 1170000, loss=0.0017602021107450128\n",
      "Iteration 1180000, loss=0.05223316326737404\n",
      "Iteration 1190000, loss=0.12021461129188538\n",
      "Iteration 1200000, loss=0.23182706534862518\n",
      "2018-06-12 04:03:15.485204\n",
      "Iteration 1210000, loss=12.445877075195312\n",
      "Iteration 1220000, loss=0.00027141670580022037\n",
      "Iteration 1230000, loss=0.12786169350147247\n",
      "Iteration 1240000, loss=3.8291826248168945\n",
      "Iteration 1250000, loss=0.36383962631225586\n",
      "Iteration 1260000, loss=0.17724482715129852\n",
      "Iteration 1270000, loss=16.11809539794922\n",
      "Iteration 1280000, loss=0.21742983162403107\n",
      "Iteration 1290000, loss=14.332947731018066\n",
      "Iteration 1300000, loss=0.5242829322814941\n",
      "2018-06-12 04:06:37.799961\n",
      "Iteration 1310000, loss=1.192093321833454e-07\n",
      "Iteration 1320000, loss=0.004181591793894768\n",
      "Iteration 1330000, loss=0.00010802716860780492\n",
      "Iteration 1340000, loss=0.016678446903824806\n",
      "Iteration 1350000, loss=0.007041075266897678\n",
      "Iteration 1360000, loss=0.0002960802521556616\n",
      "Iteration 1370000, loss=0.0009367874590680003\n",
      "Iteration 1380000, loss=0.032025132328271866\n",
      "Iteration 1390000, loss=1.4370834827423096\n",
      "Iteration 1400000, loss=4.99060583114624\n",
      "2018-06-12 04:10:07.583230\n",
      "Iteration 1410000, loss=1.192093321833454e-07\n",
      "Iteration 1420000, loss=2.691580057144165\n",
      "Iteration 1430000, loss=2.2725229263305664\n",
      "Iteration 1440000, loss=0.3542845547199249\n",
      "Iteration 1450000, loss=0.04996579512953758\n",
      "Iteration 1460000, loss=3.5762778338721546e-07\n",
      "Iteration 1470000, loss=1.9198812246322632\n",
      "Iteration 1480000, loss=1.529050588607788\n",
      "Iteration 1490000, loss=0.687286376953125\n",
      "Iteration 1500000, loss=0.5164048671722412\n",
      "2018-06-12 04:13:24.527690\n",
      "Iteration 1510000, loss=1.192093321833454e-07\n",
      "Iteration 1520000, loss=1.506039552623406e-05\n",
      "Iteration 1530000, loss=0.0008142519509419799\n",
      "Iteration 1540000, loss=1.192093321833454e-07\n",
      "Iteration 1550000, loss=0.9484527111053467\n",
      "Iteration 1560000, loss=0.5045742392539978\n",
      "Iteration 1570000, loss=1.192093321833454e-07\n",
      "Iteration 1580000, loss=5.006800165574532e-06\n",
      "Iteration 1590000, loss=0.01066772360354662\n",
      "Iteration 1600000, loss=1.192093321833454e-07\n",
      "2018-06-12 04:16:51.004070\n",
      "Iteration 1610000, loss=0.003083268878981471\n",
      "Iteration 1620000, loss=0.7770567536354065\n",
      "Iteration 1630000, loss=0.0001697684492683038\n",
      "Iteration 1640000, loss=1.192093321833454e-07\n",
      "Iteration 1650000, loss=0.03870828449726105\n",
      "Iteration 1660000, loss=0.15468420088291168\n",
      "Iteration 1670000, loss=0.15176644921302795\n",
      "Iteration 1680000, loss=0.002912460593506694\n",
      "Iteration 1690000, loss=1.3948774337768555\n",
      "Iteration 1700000, loss=1.4523717164993286\n",
      "2018-06-12 04:20:18.726778\n",
      "Iteration 1710000, loss=1.5324455499649048\n",
      "Iteration 1720000, loss=0.39515283703804016\n",
      "Iteration 1730000, loss=0.04898001626133919\n",
      "Iteration 1740000, loss=1.2201721668243408\n",
      "Iteration 1750000, loss=4.829564571380615\n",
      "Iteration 1760000, loss=0.09466655552387238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1770000, loss=6.327513694763184\n",
      "Iteration 1780000, loss=1.192093321833454e-07\n",
      "Iteration 1790000, loss=2.7557571229408495e-06\n",
      "Iteration 1800000, loss=0.2772718667984009\n",
      "2018-06-12 04:23:32.913686\n",
      "Iteration 1810000, loss=0.09646811336278915\n",
      "Iteration 1820000, loss=0.00014606586773879826\n",
      "Iteration 1830000, loss=1.6344839334487915\n",
      "Iteration 1840000, loss=8.398317337036133\n",
      "Iteration 1850000, loss=0.08305524289608002\n",
      "Iteration 1860000, loss=0.1779700368642807\n",
      "Iteration 1870000, loss=0.08573295176029205\n",
      "Iteration 1880000, loss=1.192093321833454e-07\n",
      "Iteration 1890000, loss=1.0320583581924438\n",
      "Iteration 1900000, loss=0.0005526742897927761\n",
      "2018-06-12 04:27:02.749920\n",
      "Iteration 1910000, loss=1.192093321833454e-07\n",
      "Iteration 1920000, loss=0.011460645124316216\n",
      "Iteration 1930000, loss=4.204106330871582\n",
      "Iteration 1940000, loss=0.03797717019915581\n",
      "Iteration 1950000, loss=6.0679394664475694e-05\n",
      "Iteration 1960000, loss=14.55609130859375\n",
      "Iteration 1970000, loss=0.0932709202170372\n",
      "Iteration 1980000, loss=0.08089948445558548\n",
      "Iteration 1990000, loss=0.11256449669599533\n",
      "Iteration 2000000, loss=1.192093321833454e-07\n",
      "2018-06-12 04:30:27.724194\n",
      "Iteration 2010000, loss=1.2203677892684937\n",
      "Iteration 2020000, loss=0.07927899807691574\n",
      "Iteration 2030000, loss=0.007647656369954348\n",
      "Iteration 2040000, loss=0.0014551597414538264\n",
      "Iteration 2050000, loss=2.0656614303588867\n",
      "Iteration 2060000, loss=1.2517056347860489e-05\n",
      "Iteration 2070000, loss=1.192093321833454e-07\n",
      "Iteration 2080000, loss=0.0003949855163227767\n",
      "Iteration 2090000, loss=0.00049592275172472\n",
      "Iteration 2100000, loss=2.3244580006576143e-05\n",
      "2018-06-12 04:33:46.901529\n",
      "Iteration 2110000, loss=0.40795332193374634\n",
      "Iteration 2120000, loss=3.0944616469241737e-07\n",
      "Iteration 2130000, loss=1.0000001537946446e-07\n",
      "Iteration 2140000, loss=0.002704320941120386\n",
      "Iteration 2150000, loss=0.15006013214588165\n",
      "Iteration 2160000, loss=1.8855233192443848\n",
      "Iteration 2170000, loss=0.1895240992307663\n",
      "Iteration 2180000, loss=0.015935206785798073\n",
      "Iteration 2190000, loss=1.192093321833454e-07\n",
      "Iteration 2200000, loss=1.192093321833454e-07\n",
      "2018-06-12 04:37:16.038277\n",
      "Iteration 2210000, loss=6.849759101867676\n",
      "Iteration 2220000, loss=0.09003782272338867\n",
      "Iteration 2230000, loss=0.24684061110019684\n",
      "Iteration 2240000, loss=0.2683700919151306\n",
      "Iteration 2250000, loss=0.10804837197065353\n",
      "Iteration 2260000, loss=0.0037183654494583607\n",
      "Iteration 2270000, loss=0.46456560492515564\n",
      "Iteration 2280000, loss=2.036412000656128\n",
      "Iteration 2290000, loss=0.00016938617045525461\n",
      "Iteration 2300000, loss=0.00011945480946451426\n",
      "2018-06-12 04:40:36.024385\n",
      "Iteration 2310000, loss=0.15711098909378052\n",
      "Iteration 2320000, loss=1.192093321833454e-07\n",
      "Iteration 2330000, loss=0.010451589711010456\n",
      "Iteration 2340000, loss=1.192093321833454e-07\n",
      "Iteration 2350000, loss=0.00027462979778647423\n",
      "Iteration 2360000, loss=0.0033851496409624815\n",
      "Iteration 2370000, loss=1.192093321833454e-07\n",
      "Iteration 2380000, loss=0.1134258434176445\n",
      "Iteration 2390000, loss=0.009685782715678215\n",
      "Iteration 2400000, loss=0.07288871705532074\n",
      "2018-06-12 04:44:00.107127\n",
      "Iteration 2410000, loss=4.673111470765434e-05\n",
      "Iteration 2420000, loss=1.192093321833454e-07\n",
      "Iteration 2430000, loss=0.05198045074939728\n",
      "Iteration 2440000, loss=0.000299156759865582\n",
      "Iteration 2450000, loss=9.305127143859863\n",
      "Iteration 2460000, loss=2.0051374435424805\n",
      "Iteration 2470000, loss=0.1082635223865509\n",
      "Iteration 2480000, loss=1.192093321833454e-07\n",
      "Iteration 2490000, loss=1.108652213588357e-05\n",
      "Iteration 2500000, loss=15.942384719848633\n",
      "2018-06-12 04:47:30.800024\n",
      "Iteration 2510000, loss=4.650434129871428e-05\n",
      "Iteration 2520000, loss=3.933915650122799e-06\n",
      "Iteration 2530000, loss=0.8461054563522339\n",
      "Iteration 2540000, loss=0.012283333577215672\n",
      "Iteration 2550000, loss=0.13041231036186218\n",
      "Iteration 2560000, loss=3.5405802918830886e-05\n",
      "Iteration 2570000, loss=4.649171842174837e-06\n",
      "Iteration 2580000, loss=0.019099870696663857\n",
      "Iteration 2590000, loss=1.087498048946145e-06\n",
      "Iteration 2600000, loss=0.8337358236312866\n",
      "2018-06-12 04:50:43.338458\n",
      "Iteration 2610000, loss=1.7498680353164673\n",
      "Iteration 2620000, loss=1.847760177042801e-05\n",
      "Iteration 2630000, loss=11.234466552734375\n",
      "Iteration 2640000, loss=1.192093321833454e-07\n",
      "Iteration 2650000, loss=2.1156188267923426e-06\n",
      "Iteration 2660000, loss=0.057490892708301544\n",
      "Iteration 2670000, loss=4.219154357910156\n",
      "Iteration 2680000, loss=15.005561828613281\n",
      "Iteration 2690000, loss=0.3844259977340698\n",
      "Iteration 2700000, loss=1.192093321833454e-07\n",
      "2018-06-12 04:54:11.294867\n",
      "Iteration 2710000, loss=1.25551176071167\n",
      "Iteration 2720000, loss=0.000441646232502535\n",
      "Iteration 2730000, loss=2.702143683563918e-05\n",
      "Iteration 2740000, loss=3.1293745040893555\n",
      "Iteration 2750000, loss=7.872226888139267e-07\n",
      "Iteration 2760000, loss=7.7268727181945e-05\n",
      "Iteration 2770000, loss=7.5605974197387695\n",
      "Iteration 2780000, loss=1.7223023860424291e-06\n",
      "Iteration 2790000, loss=1.919287387863733e-05\n",
      "Iteration 2800000, loss=0.011782876215875149\n",
      "2018-06-12 04:57:37.040889\n",
      "Iteration 2810000, loss=0.0016802948666736484\n",
      "Iteration 2820000, loss=0.06319893151521683\n",
      "Iteration 2830000, loss=1.192093321833454e-07\n",
      "Iteration 2840000, loss=0.17107892036437988\n",
      "Iteration 2850000, loss=1.192093321833454e-07\n",
      "Iteration 2860000, loss=16.11809539794922\n",
      "Iteration 2870000, loss=9.034629821777344\n",
      "Iteration 2880000, loss=0.014266625046730042\n",
      "Iteration 2890000, loss=15.942384719848633\n",
      "Iteration 2900000, loss=0.7528886198997498\n",
      "2018-06-12 05:00:51.689167\n",
      "Iteration 2910000, loss=0.0034494532737880945\n",
      "Iteration 2920000, loss=1.120604395866394\n",
      "Iteration 2930000, loss=1.9548377990722656\n",
      "Iteration 2940000, loss=0.03170706331729889\n",
      "Iteration 2950000, loss=0.0010915989987552166\n",
      "Iteration 2960000, loss=3.417095422744751\n",
      "Iteration 2970000, loss=1.5702980817877688e-05\n",
      "Iteration 2980000, loss=1.192093321833454e-07\n",
      "Iteration 2990000, loss=1.5497330423386302e-06\n",
      "Iteration 3000000, loss=2.3488070964813232\n",
      "2018-06-12 05:04:22.582910\n",
      "Iteration 3010000, loss=0.047350723296403885\n",
      "Iteration 3020000, loss=0.12469442933797836\n",
      "Iteration 3030000, loss=0.002373349852859974\n",
      "Iteration 3040000, loss=10.518203735351562\n",
      "Iteration 3050000, loss=9.67780876159668\n",
      "Iteration 3060000, loss=3.574199914932251\n",
      "Iteration 3070000, loss=2.4438209948129952e-05\n",
      "Iteration 3080000, loss=15.942384719848633\n",
      "Iteration 3090000, loss=1.192093321833454e-07\n",
      "Iteration 3100000, loss=1.192093321833454e-07\n",
      "2018-06-12 05:07:43.789266\n",
      "Iteration 3110000, loss=0.0016066329553723335\n",
      "Iteration 3120000, loss=0.01853235438466072\n",
      "Iteration 3130000, loss=0.029616661369800568\n",
      "Iteration 3140000, loss=1.192093321833454e-07\n",
      "Iteration 3150000, loss=8.344653110725631e-07\n",
      "Iteration 3160000, loss=0.07671253383159637\n",
      "Iteration 3170000, loss=0.48640453815460205\n",
      "Iteration 3180000, loss=3.1856205463409424\n",
      "Iteration 3190000, loss=15.8395357131958\n",
      "Iteration 3200000, loss=0.00019689236069098115\n",
      "2018-06-12 05:11:05.995309\n",
      "Iteration 3210000, loss=1.6993975639343262\n",
      "Iteration 3220000, loss=0.8311209678649902\n",
      "Iteration 3230000, loss=0.09277157485485077\n",
      "Iteration 3240000, loss=1.0354067087173462\n",
      "Iteration 3250000, loss=0.009556189179420471\n",
      "Iteration 3260000, loss=0.3521891236305237\n",
      "Iteration 3270000, loss=1.2141889333724976\n",
      "Iteration 3280000, loss=16.11809539794922\n",
      "Iteration 3290000, loss=0.0017670094966888428\n",
      "Iteration 3300000, loss=0.0007844663923606277\n",
      "2018-06-12 05:14:36.961043\n",
      "Iteration 3310000, loss=0.2525116801261902\n",
      "Iteration 3320000, loss=1.0000001537946446e-07\n",
      "Iteration 3330000, loss=0.06682129204273224\n",
      "Iteration 3340000, loss=15.942384719848633\n",
      "Iteration 3350000, loss=0.000728455139324069\n",
      "Iteration 3360000, loss=2.546491714383592e-06\n",
      "Iteration 3370000, loss=1.546515932204784e-06\n",
      "Iteration 3380000, loss=0.013726008124649525\n",
      "Iteration 3390000, loss=3.22603702545166\n",
      "Iteration 3400000, loss=0.0009476260747760534\n",
      "2018-06-12 05:17:51.139513\n",
      "Iteration 3410000, loss=0.2650463581085205\n",
      "Iteration 3420000, loss=1.0000001537946446e-07\n",
      "Iteration 3430000, loss=0.2588113248348236\n",
      "Iteration 3440000, loss=0.22006744146347046\n",
      "Iteration 3450000, loss=0.0007362599135376513\n",
      "Iteration 3460000, loss=0.0006811943021602929\n",
      "Iteration 3470000, loss=0.0040227510035037994\n",
      "Iteration 3480000, loss=0.007168171927332878\n",
      "Iteration 3490000, loss=1.5978565216064453\n",
      "Iteration 3500000, loss=5.577918273047544e-06\n",
      "2018-06-12 05:21:18.169069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3510000, loss=2.526118755340576\n",
      "Iteration 3520000, loss=1.4572294276149478e-05\n",
      "Iteration 3530000, loss=1.192093321833454e-07\n",
      "Iteration 3540000, loss=9.179154403682332e-06\n",
      "Iteration 3550000, loss=0.003248857334256172\n",
      "Iteration 3560000, loss=0.00031461307662539184\n",
      "Iteration 3570000, loss=0.40845948457717896\n",
      "Iteration 3580000, loss=0.005628319922834635\n",
      "Iteration 3590000, loss=0.6214299201965332\n",
      "Iteration 3600000, loss=15.942384719848633\n",
      "2018-06-12 05:24:46.177815\n",
      "Iteration 3610000, loss=5.545942783355713\n",
      "Iteration 3620000, loss=0.0006693457835353911\n",
      "Iteration 3630000, loss=0.008198600262403488\n",
      "Iteration 3640000, loss=1.192093321833454e-07\n",
      "Iteration 3650000, loss=11.99709415435791\n",
      "Iteration 3660000, loss=0.00031464279163628817\n",
      "Iteration 3670000, loss=0.008647171780467033\n",
      "Iteration 3680000, loss=1.0000001537946446e-07\n",
      "Iteration 3690000, loss=13.377436637878418\n",
      "Iteration 3700000, loss=0.5926105976104736\n",
      "2018-06-12 05:28:01.391837\n",
      "Iteration 3710000, loss=1.311302526119107e-06\n",
      "Iteration 3720000, loss=0.015103736892342567\n",
      "Iteration 3730000, loss=1.0000001537946446e-07\n",
      "Iteration 3740000, loss=0.1901520937681198\n",
      "Iteration 3750000, loss=2.9802374683640664e-06\n",
      "Iteration 3760000, loss=15.942384719848633\n",
      "Iteration 3770000, loss=1.192093321833454e-07\n",
      "Iteration 3780000, loss=0.0017225220799446106\n",
      "Iteration 3790000, loss=0.007603341713547707\n",
      "Iteration 3800000, loss=3.99066311729257e-06\n",
      "2018-06-12 05:31:31.694668\n",
      "Iteration 3810000, loss=0.10339362174272537\n",
      "Iteration 3820000, loss=0.03532414510846138\n",
      "Iteration 3830000, loss=0.4803558588027954\n",
      "Iteration 3840000, loss=1.023613452911377\n",
      "Iteration 3850000, loss=0.0005072720814496279\n",
      "Iteration 3860000, loss=11.022403717041016\n",
      "Iteration 3870000, loss=5.4098801612854\n",
      "Iteration 3880000, loss=0.520841658115387\n",
      "Iteration 3890000, loss=1.192093321833454e-07\n",
      "Iteration 3900000, loss=1.5370439291000366\n",
      "2018-06-12 05:34:52.367941\n",
      "Iteration 3910000, loss=2.9881601333618164\n",
      "Iteration 3920000, loss=16.11809539794922\n",
      "Iteration 3930000, loss=8.344653110725631e-07\n",
      "Iteration 3940000, loss=7.699472904205322\n",
      "Iteration 3950000, loss=0.03618195652961731\n",
      "Iteration 3960000, loss=5.714366436004639\n",
      "Iteration 3970000, loss=0.3400699496269226\n",
      "Iteration 3980000, loss=0.0011291246628388762\n",
      "Iteration 3990000, loss=1.192093321833454e-07\n",
      "Iteration 4000000, loss=13.544489860534668\n",
      "2018-06-12 05:38:11.096456\n",
      "Iteration 4010000, loss=4.203121185302734\n",
      "Iteration 4020000, loss=0.085865318775177\n",
      "Iteration 4030000, loss=3.933915650122799e-06\n",
      "Iteration 4040000, loss=1.192093321833454e-07\n",
      "Iteration 4050000, loss=2.880509614944458\n",
      "Iteration 4060000, loss=5.7512030601501465\n",
      "Iteration 4070000, loss=0.08480874449014664\n",
      "Iteration 4080000, loss=0.04448215290904045\n",
      "Iteration 4090000, loss=7.761363983154297\n",
      "Iteration 4100000, loss=7.358375072479248\n",
      "2018-06-12 05:41:42.011292\n",
      "Iteration 4110000, loss=4.649171842174837e-06\n",
      "Iteration 4120000, loss=3.4294402599334717\n",
      "Iteration 4130000, loss=0.5205994844436646\n",
      "Iteration 4140000, loss=0.025567051023244858\n",
      "Iteration 4150000, loss=0.2838790714740753\n",
      "Iteration 4160000, loss=1.2952198176208185e-06\n",
      "Iteration 4170000, loss=1.2040212823194452e-05\n",
      "Iteration 4180000, loss=0.02598736248910427\n",
      "Iteration 4190000, loss=1.192093321833454e-07\n",
      "Iteration 4200000, loss=0.0005067680031061172\n",
      "2018-06-12 05:45:01.812888\n",
      "Iteration 4210000, loss=5.7568440437316895\n",
      "Iteration 4220000, loss=14.150626182556152\n",
      "Iteration 4230000, loss=0.0004365371714811772\n",
      "Iteration 4240000, loss=11.18879508972168\n",
      "Iteration 4250000, loss=4.724575996398926\n",
      "Iteration 4260000, loss=0.0993572548031807\n",
      "Iteration 4270000, loss=0.03947024792432785\n",
      "Iteration 4280000, loss=0.07500333338975906\n",
      "Iteration 4290000, loss=0.0008676276775076985\n",
      "Iteration 4300000, loss=0.0018868353217840195\n",
      "2018-06-12 05:48:25.475705\n",
      "Iteration 4310000, loss=0.7342946529388428\n",
      "Iteration 4320000, loss=6.794950877520023e-06\n",
      "Iteration 4330000, loss=10.384306907653809\n",
      "Iteration 4340000, loss=0.001810703775845468\n",
      "Iteration 4350000, loss=0.048104964196681976\n",
      "Iteration 4360000, loss=0.2289544939994812\n",
      "Iteration 4370000, loss=15.942384719848633\n",
      "Iteration 4380000, loss=1.0280183553695679\n",
      "Iteration 4390000, loss=2.9445134714478627e-05\n",
      "Iteration 4400000, loss=0.0013145029079169035\n",
      "2018-06-12 05:51:54.655024\n",
      "Iteration 4410000, loss=6.496561050415039\n",
      "Iteration 4420000, loss=0.14500866830348969\n",
      "Iteration 4430000, loss=0.5119243264198303\n",
      "Iteration 4440000, loss=0.021379584446549416\n",
      "Iteration 4450000, loss=0.025135371834039688\n",
      "Iteration 4460000, loss=0.016717471182346344\n",
      "Iteration 4470000, loss=0.0014810198917984962\n",
      "Iteration 4480000, loss=0.09670058637857437\n",
      "Iteration 4490000, loss=1.192093321833454e-07\n",
      "Iteration 4500000, loss=6.187368869781494\n",
      "2018-06-12 05:55:07.927596\n",
      "Iteration 4510000, loss=0.00022944922966416925\n",
      "Iteration 4520000, loss=1.0000001537946446e-07\n",
      "Iteration 4530000, loss=10.737128257751465\n",
      "Iteration 4540000, loss=1.8885284662246704\n",
      "Iteration 4550000, loss=2.4080569346551783e-05\n",
      "Iteration 4560000, loss=6.633150100708008\n",
      "Iteration 4570000, loss=1.606712818145752\n",
      "Iteration 4580000, loss=2.983684778213501\n",
      "Iteration 4590000, loss=0.0005559169803746045\n",
      "Iteration 4600000, loss=1.0000001537946446e-07\n",
      "2018-06-12 05:58:37.516367\n",
      "Iteration 4610000, loss=0.01589253731071949\n",
      "Iteration 4620000, loss=0.0002835365303326398\n",
      "Iteration 4630000, loss=0.015321627259254456\n",
      "Iteration 4640000, loss=4.385974407196045\n",
      "Iteration 4650000, loss=7.152557373046875e-07\n",
      "Iteration 4660000, loss=0.08846372365951538\n",
      "Iteration 4670000, loss=0.14556097984313965\n",
      "Iteration 4680000, loss=10.401122093200684\n",
      "Iteration 4690000, loss=15.942384719848633\n",
      "Iteration 4700000, loss=6.958112716674805\n",
      "2018-06-12 06:02:02.432778\n",
      "Iteration 4710000, loss=1.0728846291385707e-06\n",
      "Iteration 4720000, loss=7.474697589874268\n",
      "Iteration 4730000, loss=3.344520092010498\n",
      "Iteration 4740000, loss=8.025467872619629\n",
      "Iteration 4750000, loss=1.0000001537946446e-07\n",
      "Iteration 4760000, loss=0.014507386833429337\n",
      "Iteration 4770000, loss=0.09899822622537613\n",
      "Iteration 4780000, loss=0.001619522226974368\n",
      "Iteration 4790000, loss=1.6867202520370483\n",
      "Iteration 4800000, loss=8.79289700606023e-07\n",
      "2018-06-12 06:05:19.124199\n",
      "Iteration 4810000, loss=0.9986871480941772\n",
      "Iteration 4820000, loss=1.917185272759525e-07\n",
      "Iteration 4830000, loss=1.192093321833454e-07\n",
      "Iteration 4840000, loss=0.013087484985589981\n",
      "Iteration 4850000, loss=10.116384506225586\n",
      "Iteration 4860000, loss=0.00012133004929637536\n",
      "Iteration 4870000, loss=1.389140038554615e-07\n",
      "Iteration 4880000, loss=0.10765304416418076\n",
      "Iteration 4890000, loss=1.192093321833454e-07\n",
      "Iteration 4900000, loss=1.7548776865005493\n",
      "2018-06-12 06:08:51.048580\n",
      "Iteration 4910000, loss=0.025437427684664726\n",
      "Iteration 4920000, loss=0.015674004331231117\n",
      "Iteration 4930000, loss=1.192093321833454e-07\n",
      "Iteration 4940000, loss=0.00025546905817463994\n",
      "Iteration 4950000, loss=0.062327876687049866\n",
      "Iteration 4960000, loss=15.942384719848633\n",
      "Iteration 4970000, loss=9.084159682970494e-05\n",
      "Iteration 4980000, loss=9.070734977722168\n",
      "Iteration 4990000, loss=1.3470737940224353e-05\n"
     ]
    }
   ],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 10000 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 100000 == 0:\n",
    "        print(datetime.datetime.now())\n",
    "     #   sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('skipgram1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00770117,  0.00796707,  0.02193469, ...,  0.02107811,\n",
       "         0.03270494,  0.04702425],\n",
       "       [ 1.275105  ,  1.24389184,  1.26958609, ...,  1.2521956 ,\n",
       "        -1.19423676,  1.1561631 ],\n",
       "       [-1.85333097, -1.7782414 , -1.83254027, ..., -1.82702792,\n",
       "         1.85137808, -1.85270727],\n",
       "       ..., \n",
       "       [-5.20829821, -4.9914012 , -5.27518845, ..., -5.30610037,\n",
       "         5.27674961, -5.27119541],\n",
       "       [ 0.2422433 ,  0.14661992,  0.20882246, ...,  0.16810475,\n",
       "        -0.21586379,  0.24917722],\n",
       "       [ 0.04168575,  0.14598228,  0.07426101, ...,  0.14002229,\n",
       "        -0.07735883,  0.10723677]], dtype=float32)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 60000 patients' data, 500000 epochs, 100 vector dimension\n",
    "#model.save('skipgram.h5')\n",
    "from keras.models import load_model\n",
    "model = load_model('skipgram.h5')\n",
    "a1 = model.layers[2].get_weights()[0]\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00770117,  0.00796707,  0.02193469, ...,  0.02107811,\n",
       "         0.03270494,  0.04702425],\n",
       "       [ 1.275105  ,  1.24389184,  1.26958609, ...,  1.2521956 ,\n",
       "        -1.19423676,  1.1561631 ],\n",
       "       [-1.85333097, -1.7782414 , -1.83254027, ..., -1.82702792,\n",
       "         1.85137808, -1.85270727],\n",
       "       ..., \n",
       "       [-5.20829821, -4.9914012 , -5.27518845, ..., -5.30610037,\n",
       "         5.27674961, -5.27119541],\n",
       "       [ 0.2422433 ,  0.14661992,  0.20882246, ...,  0.16810475,\n",
       "        -0.21586379,  0.24917722],\n",
       "       [ 0.04168575,  0.14598228,  0.07426101, ...,  0.14002229,\n",
       "        -0.07735883,  0.10723677]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 400000 patients' data, 5000000 epochs, 100 vector dimension\n",
    "#model.save('skipgram1.h5')\n",
    "from keras.models import load_model\n",
    "model = load_model('skipgram.h5')\n",
    "a1 = model.layers[2].get_weights()[0]\n",
    "a1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
